{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACKAGES LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from align import AlignDlib\n",
    "import warnings\n",
    "# Suppress LabelEncoder warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in sorted(os.listdir(path)):\n",
    "        try:\n",
    "            for f in sorted(os.listdir(os.path.join(path, i))):\n",
    "                # Check file extension. Allow only jpg/jpeg' files.\n",
    "                ext = os.path.splitext(f)[1]\n",
    "                if ext == '.jpg' or ext == '.JPG' or ext == '.jpeg':\n",
    "                    metadata.append(IdentityMetadata(path, i, f))\n",
    "        except NotADirectoryError:\n",
    "            pass\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata('images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try Haar filter as part of Viola Jones algorithm. We load a *XML* file containing Haar file descriptors for facial detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "img = cv2.imread(metadata[77].image_path())\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice this method takes a lot of time for detecting face on a single image. We will bypass this method by using a quite similar method. This method is called landmarks detection on faces through **Face Alignment**. We set the position of outer eyes and nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image of Olivier Auliard\n",
    "jc_orig = load_image(metadata[77].image_path())\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "jc_aligned = alignment.align(136, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "# Show original image\n",
    "plt.subplot(131)\n",
    "plt.imshow(jc_orig)\n",
    "\n",
    "# Show original image with bounding box\n",
    "plt.subplot(132)\n",
    "plt.imshow(jc_orig)\n",
    "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "\n",
    "# Show aligned image\n",
    "plt.subplot(133)\n",
    "plt.imshow(jc_aligned);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the OpenFace pre-trained models section, landmark indices `OUTER_EYES_AND_NOSE` are required for model nn4.small2.v1. Let's implement face detection, transformation and cropping as align_image function for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_image(img):\n",
    "    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n",
    "                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = np.zeros((metadata.shape[0], 128))\n",
    "\n",
    "for i, m in enumerate(metadata):\n",
    "    print(m)\n",
    "    img = load_image(m.image_path())\n",
    "    img = align_image(img)\n",
    "    try:\n",
    "        # scale RGB values to interval [0,1]\n",
    "        img = (img / 255.).astype(np.float32)\n",
    "        # obtain embedding vector for image\n",
    "        embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n",
    "    except TypeError:\n",
    "        print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify on a single triplet example that the squared L2 distance between its anchor-positive pair is smaller than the distance between its anchor-negative pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))\n",
    "\n",
    "def show_pair(idx1, idx2):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.suptitle(f'Distance = {distance(embedded[idx1], embedded[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata[idx2].image_path()));    \n",
    "\n",
    "show_pair(77, 76)\n",
    "show_pair(77, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the distance between the two images of Olivier Auliard is smaller than the distance between an image of Olivier Auliard and an image of Arthur Bossuet (0.30 < 1.12). But we still do not know what distance threshold $\\tau$ is the best boundary for making a decision between *same identity* and *different identity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal value for $\\tau$, the face verification performance must be evaluated on a range of distance threshold values. At a given threshold, all possible embedding vector pairs are classified as either *same identity* or *different identity* and compared to the ground truth. Since we're dealing with skewed classes (much more negative pairs than positive pairs), we use the [F1 score](https://en.wikipedia.org/wiki/F1_score) as evaluation metric instead of [accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "\n",
    "num = len(metadata)\n",
    "\n",
    "for i in range(num - 1):\n",
    "    for j in range(1, num):\n",
    "        distances.append(distance(embedded[i], embedded[j]))\n",
    "        identical.append(1 if metadata[i].name == metadata[j].name else 0)\n",
    "        \n",
    "distances = np.array(distances)\n",
    "identical = np.array(identical)\n",
    "\n",
    "thresholds = np.arange(0.3, 1.0, 0.01)\n",
    "\n",
    "f1_scores = [f1_score(identical, distances < t) for t in thresholds]\n",
    "acc_scores = [accuracy_score(identical, distances < t) for t in thresholds]\n",
    "\n",
    "opt_idx = np.argmax(f1_scores)\n",
    "# Threshold at maximal F1 score\n",
    "opt_tau = thresholds[opt_idx]\n",
    "# Accuracy at maximal F1 score\n",
    "opt_acc = accuracy_score(identical, distances < opt_tau)\n",
    "\n",
    "# Plot F1 score and accuracy as function of distance threshold\n",
    "plt.plot(thresholds, f1_scores, label='F1 score');\n",
    "plt.plot(thresholds, acc_scores, label='Accuracy');\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title(f'Accuracy at threshold {opt_tau:.2f} = {opt_acc:.3f}');\n",
    "plt.xlabel('Distance threshold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE IDENTIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = LinearSVC()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 42\n",
    "\n",
    "example_image = load_image(\"/Users/kwassibenocharlesdokodjo/Downloads/Sofiane_New.jpg\") # metadata[test_idx][example_idx].image_path()\n",
    "example_prediction = svc.predict([embedded_test])\n",
    "example_identity = encoder.inverse_transform(example_prediction)[0]\n",
    "\n",
    "plt.imshow(example_image)\n",
    "\n",
    "p = svc.decision_function(np.array([embedded_test]))\n",
    "\n",
    "probs = np.concatenate(np.exp(p)/np.sum(np.exp(p),axis=1))\n",
    "\n",
    "if probs.max()>0.3:\n",
    "    plt.title(f'Recognized as {example_identity}');\n",
    "else:\n",
    "    plt.title(f'Recognized as Unknown Person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEW SHOT PARADIGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "\n",
    "nn4_small2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(96, 96, 3))\n",
    "in_p = Input(shape=(96, 96, 3))\n",
    "in_n = Input(shape=(96, 96, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = nn4_small2(in_a)\n",
    "emb_p = nn4_small2(in_p)\n",
    "emb_n = nn4_small2(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import triplet_generator\n",
    "\n",
    "# triplet_generator() creates a generator that continuously returns \n",
    "# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n",
    "# and n_batch are batches of anchor, positive and negative RGB images \n",
    "# each having a shape of (batch_size, 96, 96, 3).\n",
    "generator = triplet_generator() \n",
    "\n",
    "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
    "nn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n",
    "\n",
    "# Please note that the current implementation of the generator only generates \n",
    "# random image data. The main goal of this code snippet is to demonstrate \n",
    "# the general setup for model training. In the following, we will anyway \n",
    "# use a pre-trained model so we don't need a generator here that operates \n",
    "# on real training data. I'll maybe provide a fully functional generator\n",
    "# later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWO FACES ON AN IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from align import AlignDlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image of Olivier Auliard\n",
    "jc_orig = load_image(\"/Users/kwassibenocharlesdokodjo/Downloads/Arthur_Charles.jpg\")\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# get all faces boundingboxes\n",
    "bb = alignment.getAllFaceBoundingBoxes(jc_orig)\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(jc_orig)\n",
    "\n",
    "jc_aligned_0 = alignment.align(136, jc_orig, bb[0], landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "jc_aligned_1 = alignment.align(136, jc_orig, bb[1], landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "\n",
    "# Show original image with bounding box\n",
    "plt.subplot(142)\n",
    "plt.imshow(jc_orig)\n",
    "plt.gca().add_patch(patches.Rectangle((bb[0].left(), bb[0].top()), bb[0].width(), bb[0].height(), fill=False, color='red'))\n",
    "plt.gca().add_patch(patches.Rectangle((bb[1].left(), bb[1].top()), bb[1].width(), bb[1].height(), fill=False, color='red'))\n",
    "\n",
    "# Show aligned image\n",
    "plt.subplot(143)\n",
    "plt.imshow(jc_aligned_0)\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(jc_aligned_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_faces=2\n",
    "\n",
    "def align_images(img, n_faces):    \n",
    "    bbs = alignment.getAllFaceBoundingBoxes(img)    \n",
    "    aligns = np.array([alignment.align(96, img, bbs[i], landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE) for i in range(n_faces)])    \n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(\"/Users/kwassibenocharlesdokodjo/Downloads/Arthur_Charles.jpg\")\n",
    "img = align_images(img, n_faces)\n",
    "try:\n",
    "    # scale RGB values to interval [0,1]\n",
    "    img = (img / 255.).astype(np.float32)\n",
    "    # obtain embedding vector for image\n",
    "    embedded_test = []\n",
    "    for i in range(n_faces):\n",
    "        embedded_test.append(nn4_small2_pretrained.predict(np.expand_dims(img[i], axis=0))[0])\n",
    "except TypeError:\n",
    "    print(\"Doesn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.25\n",
    "\n",
    "def predict_image(image_path, n_faces):\n",
    "    \n",
    "    example_image = load_image(image_path)\n",
    "    example_prediction = svc.predict(embedded_test)\n",
    "    identities = list((encoder.inverse_transform(example_prediction)))\n",
    "    probs = []\n",
    "    for i in range(n_faces):\n",
    "        p = svc.decision_function(np.array([embedded_test[i]]))\n",
    "        probs.append(np.concatenate(np.exp(p)/np.sum(np.exp(p),axis=1)))   \n",
    "    plt.imshow(example_image)\n",
    "    \n",
    "    max_values = [x.max() for x in probs]\n",
    "    \n",
    "    people = []\n",
    "    if max_values[0]>thresh:\n",
    "        people.append(identities[0])\n",
    "    else:\n",
    "        people.append('Unknown Person')\n",
    "    \n",
    "    if max_values[1]>thresh:\n",
    "        people.append(identities[1])\n",
    "    else:\n",
    "        people.append('Unknown Person')\n",
    "    \n",
    "    people = ' and '.join(people)\n",
    "    return plt.title(f'Recognized as {people}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
